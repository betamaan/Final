{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/betamaan/Final/blob/main/file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### requirements"
      ],
      "metadata": {
        "id": "zHMuSWqo_REO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langchain\n",
        "langchain-core\n",
        "\n",
        "langchain-openai\n",
        "openai\n",
        "\n",
        "langchain-anthropic\n",
        "\n",
        "langchain-google-genai\n",
        "google-generativeai\n",
        "\n",
        "langchain-huggingface\n",
        "transformers\n",
        "huggingface-hub\n",
        "\n",
        "python-dotenv\n",
        "\n",
        "numpy\n",
        "scikit-learn\n"
      ],
      "metadata": {
        "id": "04IxYvrkdmvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z22yuuuJnwZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the agents are included together.**bold text**"
      ],
      "metadata": {
        "id": "IYQj5fmcouT9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1dAdtmWnwWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "model = ChatOllama(model=\"mistral-large:123b\")\n",
        "parser = StrOutputParser()\n",
        "t_start = time.time()\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# ========== AGENT 1: Citation Numbers ==========\n",
        "print(\"\\n--- Agent 1: Extracting Citation Numbers ---\")\n",
        "loader = PyPDFLoader(\"/home/skumar/Langchain/file/paper81.pdf\")\n",
        "docs = loader.load()\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "citation_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Extract ONLY citation numbers from the academic text.\n",
        "\n",
        "Citations look like [1], [2], (3), (Smith et al., 2021), [Touvron et al., 2023], (Research, 2022), [2024], (2011)\n",
        "\n",
        "Return a plain JSON list of integers like:\n",
        "[1, 2, 3]\n",
        "\n",
        "Do not return extra text or markdown.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "chain = citation_prompt | model | parser\n",
        "all_citation_numbers = set()\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (citations)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk.page_content})\n",
        "        try:\n",
        "            numbers = json.loads(result)\n",
        "        except json.JSONDecodeError:\n",
        "            numbers = re.findall(r\"\\b\\d{1,4}\\b\", result)\n",
        "            numbers = list(map(int, numbers))\n",
        "        all_citation_numbers.update(numbers)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "candidate_citations = sorted(n for n in all_citation_numbers if 1 <= int(n) <= 300)\n",
        "\n",
        "verified_citations = set()\n",
        "for n in candidate_citations:\n",
        "    pattern_square = rf\"\\[{n}\\]\"\n",
        "    pattern_round = rf\"\\({n}\\)\"\n",
        "    for chunk in chunks:\n",
        "        if re.search(pattern_square, chunk.page_content) or re.search(pattern_round, chunk.page_content):\n",
        "            verified_citations.add(n)\n",
        "            break\n",
        "\n",
        "filtered_citation_numbers = sorted(verified_citations)\n",
        "\n",
        "with open(\"results/citation_numbers.json\", \"w\") as f:\n",
        "    json.dump(filtered_citation_numbers, f)\n",
        "\n",
        "with open(\"results/chunks.json\", \"w\") as f:\n",
        "    json.dump([{\"page_content\": chunk.page_content} for chunk in chunks], f)\n",
        "\n",
        "print(\"Total citations found:\", len(filtered_citation_numbers))\n",
        "t1 = time.time()\n",
        "print(f\"Agent 1 time: {t1 - t_start:.2f} sec\")\n",
        "\n",
        "# ========== AGENT 2: Citation Authors ==========\n",
        "print(\"\\n--- Agent 2: Extracting Citation Authors ---\")\n",
        "with open(\"results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "with open(\"results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "author_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a section of an academic paper. Extract a list of properly formatted citation entries.\n",
        "\n",
        "Each entry must have:\n",
        "- The citation number\n",
        "- The full author list\n",
        "\n",
        "Format each citation like this:\n",
        "Citation No. 1: Kumar R., Sharma V.\n",
        "\n",
        "Return as JSON list:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"author\": \"Kumar R., Sharma V.\"}},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Only include entries with author. Do not return anything else.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = author_prompt | model | parser\n",
        "final_citations = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (authors)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            author = entry.get(\"author\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                author and\n",
        "                \"not available\" not in author.lower() and\n",
        "                \"n/a\" not in author.lower()\n",
        "            ):\n",
        "                final_citations[int(cnum)] = author\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "with open(\"results/clean_citations.json\", \"w\") as f:\n",
        "    json.dump(final_citations, f, indent=2)\n",
        "\n",
        "print(\"\\nExtracted Citation Authors:\")\n",
        "for cnum in sorted(final_citations):\n",
        "    print(f\"Citation {cnum}: {final_citations[cnum]}\")\n",
        "\n",
        "t2 = time.time()\n",
        "print(f\"Agent 2 time: {t2 - t1:.2f} sec\")\n",
        "\n",
        "# ========== AGENT 3: Citation Years + Timeline ==========\n",
        "print(\"\\n--- Agent 3: Extracting Citation Years ---\")\n",
        "\n",
        "year_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are given a chunk of an academic research paper.\n",
        "\n",
        "Your task is to find the publication year for each in-text citation.\n",
        "Only return years if they are clearly associated with a citation.\n",
        "\n",
        "Return the result in this JSON format:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"year\": \"2023\"}},\n",
        "  {{\"citation_no\": 2, \"year\": \"2020\"}}\n",
        "]\n",
        "\n",
        "Do not include citations without a year. Skip missing or unclear entries.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = year_prompt | model | parser\n",
        "citation_years = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (years)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            year = entry.get(\"year\")\n",
        "\n",
        "            if cnum in citation_numbers:\n",
        "                if year and re.match(r\"^(19|20)\\d{2}$\", str(year)):\n",
        "                    citation_years[int(cnum)] = year\n",
        "                else:\n",
        "                    # Add as unknown temporarily; we may overwrite later if found in other chunks\n",
        "                    citation_years[int(cnum)] = \"Unknown\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "for cnum in citation_numbers:\n",
        "    if int(cnum) not in citation_years:\n",
        "        citation_years[int(cnum)] = \"Unknown\"\n",
        "\n",
        "with open(\"results/years.json\", \"w\") as f:\n",
        "    json.dump(citation_years, f, indent=2)\n",
        "\n",
        "print(f\"\\nTotal citation years extracted: {len(citation_years)}\")\n",
        "\n",
        "# Timeline plotting\n",
        "def draw_basic_timeline(years_json_path, figSize=(15, 10)):\n",
        "    with open(years_json_path, 'r') as f:\n",
        "        years_map = json.load(f)\n",
        "\n",
        "    # citations_with_years = []\n",
        "    # for num_str, year_str in years_map.items():\n",
        "    #     try:\n",
        "    #         citations_with_years.append({\"number\": num_str, \"year\": int(year_str)})\n",
        "    #     except ValueError:\n",
        "    #         continue\n",
        "    citations_with_years = [\n",
        "        {\"number\": num_str, \"year\": int(year_str)}\n",
        "        for num_str, year_str in years_map.items()\n",
        "        if str(year_str).isdigit()\n",
        "    ]\n",
        "\n",
        "    citations_with_years.sort(key=lambda x: x['year'])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figSize)\n",
        "    min_year = min(c['year'] for c in citations_with_years) - 1\n",
        "    max_year = max(c['year'] for c in citations_with_years) + 1\n",
        "\n",
        "    ax.hlines(0, min_year, max_year, color='gray', linestyle='-', linewidth=1.5)\n",
        "    ax.set_xticks(range(min_year, max_year + 1))\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.set_xlim(min_year, max_year)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.spines[['left', 'right', 'top']].set_visible(False)\n",
        "    ax.spines['bottom'].set_linewidth(1.5)\n",
        "\n",
        "    y_offset_factor = 0.1\n",
        "    y_positions = {}\n",
        "\n",
        "    for citation in citations_with_years:\n",
        "        year = citation['year']\n",
        "        label = f\"#{citation['number']}\"\n",
        "        current_y_offset = y_positions.get(year, 0)\n",
        "\n",
        "        # y_pos = y_offset_factor * ((current_y_offset + 1) * (-1 if current_y_offset % 2 else 1))\n",
        "        # y_positions[year] = current_y_offset + 1\n",
        "\n",
        "        if year % 2 == 0:\n",
        "            y_pos = y_offset_factor * ((current_y_offset  + 0.5 // 2) * (1 if current_y_offset % 2 == 0 else -1))\n",
        "            y_positions[year] = current_y_offset + 1\n",
        "        else:\n",
        "            y_pos = y_offset_factor * ((current_y_offset - 0.5 // 2 + 1) * (1 if current_y_offset % 2 == 0 else -1))\n",
        "            y_positions[year] = current_y_offset + 1\n",
        "\n",
        "        ax.plot(year, 0, 'o', color='darkblue', markersize=6)\n",
        "        ax.plot([year, year], [0, y_pos], color='skyblue', linestyle='--', linewidth=0.8)\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            xy=(year, y_pos),\n",
        "            xytext=(year, y_pos + (0.02 if y_pos > 0 else -0.02)),\n",
        "            fontsize=9,\n",
        "            ha='center',\n",
        "            va='bottom' if y_pos > 0 else 'top',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
        "            arrowprops=dict(arrowstyle=\"-\", color='gray', linewidth=0.5)\n",
        "        )\n",
        "\n",
        "    plt.title(\"Citation Timeline\", fontsize=14, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    try:\n",
        "        output_path = Path(years_json_path).with_name(\"Basic_Citations_Timeline.jpeg\")\n",
        "        fig.savefig(output_path, format=\"jpeg\", bbox_inches=\"tight\", dpi=300)\n",
        "        print(f\"Timeline image saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save image: {e}\")\n",
        "\n",
        "draw_basic_timeline(\"results/years.json\")\n",
        "t3 = time.time()\n",
        "print(f\"Agent 3 time: {t3 - t2:.2f} sec\")\n",
        "print(f\"\\n All agents completed in {t3 - t_start:.2f} seconds.\")\n",
        "\n",
        "\n",
        "# ========== AGENT 4: Citation Summaries and Clustering ==========\n",
        "print(\"\\n--- Agent 4: Summarizing and Clustering Citations ---\")\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load previously extracted citation numbers\n",
        "with open(\"results/citation_numbers.json\") as f:\n",
        "    citation_numbers = json.load(f)\n",
        "\n",
        "# Load chunks\n",
        "with open(\"results/chunks.json\") as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "Given this chunk of a research paper, generate a unique 2-line summary for each citation.\n",
        "\n",
        "Format as a JSON list:\n",
        "[\n",
        "  {{\"citation_no\": 1, \"summary\": \"This study explored ...\"}},\n",
        "  {{\"citation_no\": 2, \"summary\": \"Authors evaluated ...\"}}\n",
        "]\n",
        "\n",
        "Do not repeat the same summary.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = summary_prompt | model | parser\n",
        "\n",
        "citation_summaries = {}\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Processing chunk {i+1} (summaries)\")\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": chunk[\"page_content\"]})\n",
        "        match = re.search(r\"\\[.*\\]\", result.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "        data = json.loads(match.group())\n",
        "        for entry in data:\n",
        "            cnum = entry.get(\"citation_no\")\n",
        "            summary = entry.get(\"summary\")\n",
        "            if (\n",
        "                cnum in citation_numbers and\n",
        "                summary and\n",
        "                \"not available\" not in summary.lower() and\n",
        "                \"n/a\" not in summary.lower() and\n",
        "                int(cnum) not in citation_summaries\n",
        "            ):\n",
        "                citation_summaries[int(cnum)] = summary.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in chunk {i+1}: {e}\")\n",
        "\n",
        "# Save summaries\n",
        "with open(\"results/summaries.json\", \"w\") as f:\n",
        "    json.dump(citation_summaries, f, indent=2)\n",
        "\n",
        "# Embedding and clustering\n",
        "citation_ids = list(citation_summaries.keys())\n",
        "citation_texts = list(citation_summaries.values())\n",
        "\n",
        "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
        "embeddings = embedding_model.embed_documents(citation_texts)\n",
        "\n",
        "# Elbow method\n",
        "k_range = range(2, 11)\n",
        "inertias = []\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(embeddings)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "def find_elbow_point(inertias):\n",
        "    diffs = [inertias[i] - inertias[i+1] for i in range(len(inertias)-1)]\n",
        "    slopes = [diffs[i] - diffs[i+1] for i in range(len(diffs)-1)]\n",
        "    return slopes.index(max(slopes)) + 2\n",
        "\n",
        "optimal_k = find_elbow_point(inertias)\n",
        "print(f\"Optimal k (by elbow method): {optimal_k}\")\n",
        "\n",
        "# Cluster\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "clusters_raw = defaultdict(list)\n",
        "for idx, label in enumerate(labels):\n",
        "    clusters_raw[label].append((citation_ids[idx], citation_texts[idx]))\n",
        "\n",
        "cluster_summaries = []\n",
        "for i, entries in clusters_raw.items():\n",
        "    cluster_text = \"\\n\".join([f\"[{cid}] {text}\" for cid, text in entries])\n",
        "    cluster_summaries.append((f\"Cluster {i+1}\", cluster_text))\n",
        "\n",
        "# Subgrouping via LLM\n",
        "cluster_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a helpful assistant. You are given a group of research summaries, each prefixed by a citation number like [22].\n",
        "\n",
        "Your task is to analyze this group and organize it into labeled subgroups based on theme, technique, or topic.\n",
        "Return the result as a JSON object with structure like:\n",
        "\n",
        "{{\n",
        "  \"Main Theme of This Cluster\": {{\n",
        "    \"Subgroup A\": [22, 23],\n",
        "    \"Subgroup B\": [24, 25]\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Only use citation numbers in the output. Do not include summaries or explanations.\n",
        "\n",
        "Cluster label: {label}\n",
        "Entries:\n",
        "{text}\n",
        "\"\"\",\n",
        "    input_variables=[\"label\", \"text\"]\n",
        ")\n",
        "\n",
        "final_clusters = {}\n",
        "for label, cluster_text in cluster_summaries:\n",
        "    try:\n",
        "        result = (cluster_prompt | model | parser).invoke({\"label\": label, \"text\": cluster_text})\n",
        "        parsed = json.loads(result[result.find(\"{\"):])\n",
        "        final_clusters[label] = parsed\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {label}: {e}\")\n",
        "\n",
        "with open(\"results/hybrid_llm_embedding_clusters.json\", \"w\") as f:\n",
        "    json.dump(final_clusters, f, indent=2)\n",
        "print(\"Hybrid clustering complete.\")\n",
        "\n",
        "### ========== Output Representation ==========\n",
        "\n",
        "# D3 Hierarchy Builder\n",
        "def build_hierarchy(data):\n",
        "    children = []\n",
        "    for cluster, structure in data.items():\n",
        "        theme = list(structure.keys())[0]\n",
        "        cluster_node = {\"name\": cluster, \"children\": []}\n",
        "        for subgroup, citations in structure[theme].items():\n",
        "            subgroup_node = {\"name\": subgroup, \"children\": [{\"name\": f\"Citation {c}\"} for c in citations]}\n",
        "            cluster_node[\"children\"].append(subgroup_node)\n",
        "        children.append(cluster_node)\n",
        "    return {\"name\": \"Root\", \"children\": children}\n",
        "\n",
        "hierarchy = build_hierarchy(final_clusters)\n",
        "\n",
        "with open(\"results/d3_citation_tree.json\", \"w\") as f:\n",
        "    json.dump(hierarchy, f, indent=2)\n",
        "\n",
        "print(\"D3-compatible JSON saved to results/d3_citation_tree.json\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FGgQ-AxAnwT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVZA3W4BkvN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For representation\n",
        "(IN HTML format)"
      ],
      "metadata": {
        "id": "FBlCu2oQpGdm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftRYzOcgpmCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>Citation Cluster Visualization</title>\n",
        "  <style>\n",
        "    .node circle {\n",
        "      fill: #1f77b4;\n",
        "    }\n",
        "\n",
        "    .node text {\n",
        "      font: 12px sans-serif;\n",
        "    }\n",
        "\n",
        "    .link {\n",
        "      fill: none;\n",
        "      stroke: #ccc;\n",
        "      stroke-width: 2px;\n",
        "    }\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <h2>Citation Clustering Hierarchy (D3.js)</h2>\n",
        "  <svg width=\"1800\" height=\"800\"></svg>\n",
        "\n",
        "  <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n",
        "  <script>\n",
        "    const svg = d3.select(\"svg\"),\n",
        "          width = +svg.attr(\"width\"),\n",
        "          height = +svg.attr(\"height\");\n",
        "\n",
        "    const g = svg.append(\"g\").attr(\"transform\", \"translate(120, 80)\");\n",
        "\n",
        "    const tree = d3.tree().size([height - 100, width - 300]);\n",
        "\n",
        "    d3.json(\"d3_citation_tree.json\").then(data => {\n",
        "      const root = d3.hierarchy(data);\n",
        "      tree(root);\n",
        "\n",
        "      // Draw links\n",
        "      const link = g.selectAll(\".link\")\n",
        "          .data(root.links())\n",
        "          .enter().append(\"path\")\n",
        "          .attr(\"class\", \"link\")\n",
        "          .attr(\"d\", d3.linkHorizontal()\n",
        "                      .x(d => d.y)\n",
        "                      .y(d => d.x));\n",
        "\n",
        "      // Draw nodes\n",
        "      const node = g.selectAll(\".node\")\n",
        "          .data(root.descendants())\n",
        "          .enter().append(\"g\")\n",
        "          .attr(\"class\", \"node\")\n",
        "          .attr(\"transform\", d => `translate(${d.y},${d.x})`);\n",
        "\n",
        "      node.append(\"circle\")\n",
        "          .attr(\"r\", 6);\n",
        "\n",
        "      node.append(\"text\")\n",
        "          .attr(\"dy\", 3)\n",
        "          .attr(\"x\", d => d.children ? -10 : 10)\n",
        "          .style(\"text-anchor\", d => d.children ? \"end\" : \"start\")\n",
        "          .text(d => d.data.name);\n",
        "    });\n",
        "  </script>\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!-- cd your_project\n",
        "python3 -m http.server\n",
        "http://localhost:8000\n",
        " -->\n"
      ],
      "metadata": {
        "id": "uXkwH446pF2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}